#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
  Manages multipart uploads to s3.
  
"""
import os
import argparse
from cStringIO import StringIO
import logging
from math import ceil
from multiprocessing import Pool
import time
import urlparse
import boto
import os
from boto.s3.connection import S3Connection

parser = argparse.ArgumentParser(description="Transfer large files to S3", prog="nmdfile")
parser.add_argument("src", type=file, help="The file to transfer")
parser.add_argument("dest", help="The S3 destination object")
parser.add_argument("-np", "--num-processes", help="Number of processors to use", type=int, default=2)
parser.add_argument("-f", "--force", help="Overwrite an existing S3 key", action="store_true")
parser.add_argument("-s", "--split", help="Split size, in Mb", type=int, default=50)
parser.add_argument("-rrs", "--reduced-redundancy", help="Use reduced redundancy storage. Default is standard.", default=False,  action="store_true")
parser.add_argument("--insecure", dest='secure', help="Use HTTP for connection", default=True, action="store_false")
parser.add_argument("-t", "--max-tries", help="Max allowed retries for http timeout", type=int, default=5)
parser.add_argument("-k", "--key", help="AWS ACCESS KEY", dest='aws_access_key')
parser.add_argument("-sk", "--secret", help="AWS SECRET KEY", dest='aws_secret_key')
parser.add_argument("-l", "--log", help="Log Level", default='INFO')
aws_key=''
aws_secret=''

def main(src, dest, num_processes=2, split=50, force=False, reduced_redundancy=False, verbose=False, quiet=False, secure=True, max_tries=5, aws_access_key='', aws_secret_key='', log='info'):
  """This is the main function for the s3upload module.
 
  :param obj src: open file resource object pointing to the local file being uploaded.
  :param str dest: a string containing the full s3:// path the file is being created at.
  :param int num_processes: Number of processors to use.
  :param int split: Split size, in Mb.
  :param bool force: Overwrite an existing S3 key.
  :param bool reduced_redundancy: Use reduced redundancy storage.
  :param bool verbose: Be more verbose.
  :param bool quiet: Be less verbose.
  """
  global aws_key
  aws_key = aws_access_key

  global aws_secret
  aws_secret = aws_secret_key

  if type(src) is str:
    src = open(src)

  split_rs = urlparse.urlsplit(dest)

  if split_rs.scheme != "s3":
    raise ValueError("'%s' is not an S3 url" % dest)

  s3 = S3Connection(aws_key, aws_secret)
  bucket = s3.get_bucket(split_rs.netloc)

  if bucket == None:
      raise ValueError("'%s' is not a valid bucket" % split_rs.netloc)

  key = bucket.get_key(split_rs.path)

  if key is not None:
    if not force:
      raise ValueError("'%s' already exists. Specify -f to overwrite it" % dest)

  part_size = max(5*1024*1024, 1024*1024*split)
  src.seek(0,2)
  size = src.tell()
  num_parts = int(ceil(size / part_size))

  if size < 5*1024*1024:
    src.seek(0)
    t1 = time.time()
    k = boto.s3.key.Key(bucket,split_rs.path)
    k.set_contents_from_file(src)
    t2 = time.time() - t1
    s = size/1024./1024.
    logging.info("Finished uploading %0.2fM in %0.2fs (%0.2fMBps)" % (s, t2, s/t2))
    return

  mpu = bucket.initiate_multipart_upload(split_rs.path, reduced_redundancy=reduced_redundancy, encrypt_key=True)
  logging.info("Initialized upload: %s" % mpu.id)

  def gen_args(num_parts, fold_last):
    for i in range(num_parts+1):
      part_start = part_size*i
      if i == (num_parts-1) and fold_last is True:
        yield (bucket.name, mpu.id, src.name, i, part_start, part_size*2, secure, max_tries, 0)
        break
      else:
        yield (bucket.name, mpu.id, src.name, i, part_start, part_size, secure, max_tries, 0)

  fold_last = ((size % part_size) < 5*1024*1024)

  try:
      pool = Pool(processes=num_processes)
      t1 = time.time()
      pool.map_async(do_part_upload, gen_args(num_parts, fold_last)).get(9999999)
      t2 = time.time() - t1
      s = size/1024./1024.
      src.close()
      mpu.complete_upload()
      logging.info("Finished uploading %0.2fM in %0.2fs (%0.2fMBps)" % (s, t2, s/t2))
  except KeyboardInterrupt:
      logging.warn("Received KeyboardInterrupt, canceling upload")
      pool.terminate()
      mpu.cancel_upload()
  except Exception, err:
      logging.error("Encountered an error, canceling upload")
      logging.error(err)
      mpu.cancel_upload()

def do_part_upload(args):
  """
  Upload a part of a MultiPartUpload

  Open the target file and read in a chunk. Since we can't pickle
  S3Connection or MultiPartUpload objects, we have to reconnect and lookup
  the MPU object with each part upload.

  :type args: tuple of (string, string, string, int, int, int)
  :param args: The actual arguments of this method. Due to lameness of
               multiprocessing, we have to extract these outside of the
               function definition.

               The arguments are: S3 Bucket name, MultiPartUpload id, file
               name, the part number, part offset, part size
  """
  bucket_name, mpu_id, fname, i, start, size, secure, max_tries, current_tries = args
  logging.debug("do_part_upload got args: %s" % (args,))

  s3 = S3Connection(aws_key, aws_secret)
  bucket = s3.get_bucket('nmdarchive')
  mpu = None

  for mp in bucket.list_multipart_uploads():
    if mp.id == mpu_id:
      mpu = mp
      break

  if mpu is None:
    raise Exception("Could not find MultiPartUpload %s" % mpu_id)

  fp = open(fname, 'rb')
  fp.seek(start)
  data = fp.read(size)
  fp.close()

  if not data:
    raise Exception("Unexpectedly tried to read an empty chunk")

  def progress(x,y):
    logging.debug("Part %d: %0.2f%%" % (i+1, 100.*x/y))

  try:
    t1 = time.time()
    mpu.upload_part_from_file(StringIO(data), i+1, cb=progress)
    t2 = time.time() - t1
    s = len(data)/1024./1024.
    logging.info("Uploaded part %s (%0.2fM) in %0.2fs at %0.2fMBps" % (i+1, s, t2, s/t2))
  except Exception, err:
    logging.debug("Retry request %d of max %d times" % (current_tries, max_tries))
    if (current_tries > max_tries):
      logging.error(err)
    else:
      time.sleep(3)
      current_tries += 1
      do_part_download(bucket_name, mpu_id, fname, i, start, size, secure, max_tries, current_tries)

if __name__ == "__main__":
  args = parser.parse_args()
  logger = logging.getLogger()
  exec("logger.setLevel(logging." + args.log.upper() + ")")

  # create console handler and set level to NMD_LOG
  handler = logging.StreamHandler()
  exec("handler.setLevel(logging." + args.log.upper() + ")")
  formatter = logging.Formatter("%(levelname)s - %(message)s")
  handler.setFormatter(formatter)
  logger.addHandler(handler)

#  logging.basicConfig(level=logging.INFO)
  arg_dict = vars(args)
  logging.debug("CLI args: %s" % args)
  main(**arg_dict)
